import json
import re

from django.conf import settings
from django.core.cache import cache, caches

#! –º–µ—Ç–æ–¥ –¥–ª—è messages.json 
async def get_message_text(message_key: str, version: str, language: str='ru') -> str:
    if not settings.DEBUG:
        messages = await caches['redis'].aget('messages')
    else:
        messages=None    
    if not messages:
        # –ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏–π –∏–∑ JSON-—Ñ–∞–π–ª–∞
        with open('messages.json', 'r', encoding='utf-8') as file:
            messages = json.load(file)
            await caches['redis'].aset('messages', messages, None) # –°—Ç–∞–≤–∏–º –≤ –∫—ç—à –Ω–∞–≤—Å–µ–≥–¥–∞

    return messages.get(language, {}).get(message_key, {}).get(version, "Message not found")  

#! –¢–µ–∫—Å—Ç –ø–æ—Å—Ç–æ–≤
def anketa_text(title, description, count_people, city=None, likes=0, dislikes=0):
    text= f'''
> <b>{title}</b>
{f'\n‚äπ <i>{description}</i> ‚äπ\n' if description else ''}
{f'\nüåÜ {city}' if city else ''}
üë• <b>{count_people}</b>   üíñ <b>{likes}</b>   üëé <b>{dislikes}</b>

(—Å) –ù–ê—à –∫—Ä—É—Ç–æ–π –±–æ—Ç 
        '''      
    return text

#! –õ–æ–≥–∏–∫–∞ —Ñ–∏–ª—Ç—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞
async def ban_words_cheking(text):

    # –†–µ–≥—É–ª—è—Ä–Ω–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å—Å—ã–ª–æ–∫
    link_pattern = r"(?i)(https?://[^\s<>\"']+|www\d{0,3}\.[^\s<>\"']+|t\.me/[^\s<>\"']+|@[\w\d_]+)"
    if re.search(link_pattern, text):
        return await get_message_text("errors", "ban_link")

    ban_words = await cache.aget("ban_words")
    if not ban_words:
        # –í—ã–≥—Ä—É–∂–∞–µ–º —Ñ–∞–π–ª —Å –∑–∞–ø—Ä–µ—Ç–∫–∞–º–∏
        with open("ban_words.json", "r", encoding="utf-8") as file:
            data = json.load(file)
            ban_words = set(data.get("prohibited_words", []))  # –ò—Å–ø–æ–ª—å–∑—É–µ–º set –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
            await cache.aset("ban_words", ban_words, None)       

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –∑–∞–ø—Ä–µ—â—ë–Ω–Ω—ã—Ö —Å–ª–æ–≤
    found_words = [word for word in ban_words if word.lower() in text.lower()]
    if found_words:
        return await get_message_text("errors", "ban_words_text") + f"{', '.join(found_words)}."
        
    return None  
#! –í—ã—Ç–∞—Å–∫–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ –∞–Ω–∫–µ—Ç—ã   
async def extract_text(post_text, start_token, end_token):
    start = post_text.find(start_token)
    end = post_text.find(end_token, start)

    if start != -1 and end != -1:
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ –º–µ–∂–¥—É –º–∞—Ä–∫–µ—Ä–∞–º–∏
        return post_text[start + len(start_token):end].strip()
    return None   
async def extract_link(text, start_symbols='t.me/'):
    # –î–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Å—ã–ª–∫–∏ –≤ –∞–Ω–∫–µ—Ç–µ
    for line in text.splitlines():
        if line.startswith(start_symbols):
            return line[len(start_symbols):].split()[0]
    return None